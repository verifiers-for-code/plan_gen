{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = load_dataset('verifiers-for-code/merged-227k', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = load_dataset('verifiers-for-code/CodePython-27k-multiple-plangen', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dataset to pandas DataFrame\n",
    "def dataset_to_dataframe(dataset):\n",
    "    return pd.DataFrame({col: dataset[col] for col in dataset.features})\n",
    "\n",
    "# Convert pandas DataFrame back to Dataset\n",
    "def dataframe_to_dataset(df):\n",
    "    return Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "main_df = dataset_to_dataframe(main)\n",
    "merge1_df = dataset_to_dataframe(merge1)\n",
    "\n",
    "# Merge datasets\n",
    "merged = pd.merge(main_df, merge1_df, left_on=['problem', 'solution'], right_on=['input', 'code'], how='inner')\n",
    "\n",
    "# Create additional rows for other plan types\n",
    "plan_columns = [\n",
    "    'non_granular_plans_Llama-3-70B',\n",
    "    'granular_plans_Llama-3-70B',\n",
    "    'non_granular_plans_temp_0_dot_7_llama_3_70B',\n",
    "    'non_granular_plans_temp_0_llama_3_70B'\n",
    "]\n",
    "\n",
    "additional_rows = []\n",
    "\n",
    "for _, row in merged.iterrows():\n",
    "    for plan_col in plan_columns:\n",
    "        new_row = row[['problem', 'solution']].copy()\n",
    "        new_row['70B_plans'] = row[plan_col]\n",
    "        additional_rows.append(new_row)\n",
    "\n",
    "# Create a DataFrame from additional rows\n",
    "additional_df = pd.DataFrame(additional_rows)\n",
    "\n",
    "# Concatenate the original main dataset with the additional rows\n",
    "final_df = pd.concat([main_df, additional_df], ignore_index=True)\n",
    "\n",
    "# Convert the final DataFrame back to a Dataset\n",
    "final_dataset = dataframe_to_dataset(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', '70B_plans'],\n",
       "    num_rows: 336120\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2 = load_dataset('verifiers-for-code/Python-Alpaca-18k-plangen', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input', 'non_granular_plans_Llama_3_70B'],\n",
       "    num_rows: 18534\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2 = merge2.rename_column(\"input\", \"problem\")\n",
    "merge2 = merge2.rename_column(\"output\", \"solution\")\n",
    "merge2 = merge2.rename_column(\"non_granular_plans_Llama_3_70B\", \"70B_plans\")\n",
    "\n",
    "# Remove 'instruction' column from merge2\n",
    "merge2 = merge2.remove_columns([\"instruction\"])\n",
    "\n",
    "# Concatenate datasets\n",
    "combined_dataset = concatenate_datasets([final_dataset, merge2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', '70B_plans'],\n",
       "    num_rows: 354654\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge3 = load_dataset('verifiers-for-code/tester3', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data_name', 'id', 'prompt', 'code', 'text', 'input', 'non_granular_plans_Llama-3-70B', 'granular_plans_Llama-3-70B', 'deepseek_plans_eval', 'deepseek_solution_eval', 'non_granular_plans_temp_0_dot_7_llama_3_70B', 'non_granular_plans_temp_0_llama_3_70B', 'structured_plans_Llama-3_1-70B'],\n",
       "    num_rows: 27224\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge3 = merge3.rename_column('input', 'problem')\n",
    "merge3 = merge3.rename_column('code', 'solution')\n",
    "merge3 = merge3.rename_column('structured_plans_Llama-3_1-70B', '70B_plans')\n",
    "\n",
    "# Select only the columns we need from merge3\n",
    "merge3_subset = merge3.select_columns(['problem', 'solution', '70B_plans'])\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined2 = concatenate_datasets([combined_dataset, merge3_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', '70B_plans'],\n",
       "    num_rows: 381878\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge4 = load_dataset('verifiers-for-code/cleaned_deepseek_plans', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data_name', 'id', 'prompt', 'code', 'text', 'input', 'generated_plans_DeepSeek-Coder-V2-Instruct'],\n",
       "    num_rows: 27224\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge4 = merge4.rename_column('input', 'problem')\n",
    "merge4 = merge4.rename_column('code', 'solution')\n",
    "merge4 = merge4.rename_column('generated_plans_DeepSeek-Coder-V2-Instruct', '70B_plans')\n",
    "\n",
    "# Select only the columns we need from merge3\n",
    "merge4_subset = merge4.select_columns(['problem', 'solution', '70B_plans'])\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined3 = concatenate_datasets([combined2, merge4_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', '70B_plans'],\n",
       "    num_rows: 409102\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 481/481 [00:00<00:00, 1.59MB/s]\n",
      "Downloading data: 100%|██████████| 41.3M/41.3M [00:03<00:00, 11.0MB/s]\n",
      "Generating train split: 100%|██████████| 10000/10000 [00:00<00:00, 26642.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "merge5 = load_dataset('verifiers-for-code/sampled_10k_from_227k', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', 'gpt-4o-mini-plans', 'text', 'text_gemma', 'text_nosys_phi'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge5 = merge5.rename_column('gpt-4o-mini-plans', '70B_plans')\n",
    "\n",
    "# Select only the columns we need from merge3\n",
    "merge5_subset = merge5.select_columns(['problem', 'solution', '70B_plans'])\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined4 = concatenate_datasets([combined3, merge4_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', '70B_plans'],\n",
       "    num_rows: 436326\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 146/146 [00:00<00:00, 171.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 146/146 [00:00<00:00, 147.36ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 146/146 [00:02<00:00, 71.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [00:47<00:00, 15.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/verifiers-for-code/merged/commit/5342cf07ac7d04b0b66ce5d11a9c02e44cf57402', commit_message='Upload dataset', commit_description='', oid='5342cf07ac7d04b0b66ce5d11a9c02e44cf57402', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined4.push_to_hub(\"verifiers-for-code/merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
